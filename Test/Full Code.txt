I have this code for loading a MP4 video and counting people in an ROI based off a YOLO model.
See code:
#@title 1: Install ultralytics for YOLO library
!pip install ultralytics

#@title 1.1: Loading related libraries
import cv2
from ultralytics import YOLO

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np
import os
import subprocess
from tqdm.notebook import tqdm

import IPython
from IPython.display import Video, display
%matplotlib inline
import urllib.request
import shutil

#@title 2: Loading a YOLO model
model = YOLO('yolov8x.pt')

# Enhance the YOLO model with additional knowledge transfer to help with unique lighting or busy ROI
model.train(data="coco128.yaml", epochs=2)  # train the model

#geting names from classes
dict_classes = model.model.names

#@title 2.1: Load video (emulate capture)
# Video  path for experiment
path = '/content/drive/MyDrive/AAI-521/Final/Test/City_Hall-IOT1.mp4'

#@title 3: Define required functions for video processing
# process frames
def risize_frame(frame, scale_percent):
    """Function to resize frame"""
    # resize image
    width = int(frame.shape[1] * scale_percent / 100)
    height = int(frame.shape[0] * scale_percent / 100)
    dim = (width, height)
    # resize image
    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)
    return resized

# Filter the history of tracked objects
def filter_tracks(centers, patience):
    """Function to filter the history of tracked objects"""
    filter_dict = {}
    for k, i in centers.items():
        d_frames = i.items()
        filter_dict[k] = dict(list(d_frames)[-patience:])

    return filter_dict

# Update tracked objects
def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):
    is_new = 0
    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]
    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]
    # Calculating distance from existing centers points
    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]
    # if distance less than a threshold, it will update its positions
    if previous_pos:
        id_obj = previous_pos[0][0]
        centers_old[id_obj][frame] = obj_center

    # Else a new ID will be set to the given object
    else:
        if lastKey:
            last = lastKey.split('D')[1]
            id_obj = 'ID' + str(int(last)+1)
        else:
            id_obj = 'ID0'

        is_new = 1
        centers_old[id_obj] = {frame:obj_center}
        lastKey = list(centers_old.keys())[-1]

    return centers_old, id_obj, is_new, lastKey

#@title 3.1: Configurations for ROI objects
#Verbose during prediction
verbose = False
# Scaling percentage of original frame
scale_percent = 100
# model confidence level
conf_level = 0.8
# Threshold of centers ( old\new)
thr_centers = 20
#Number of max frames to consider a object lost
frame_max = 5
# Number of max tracked centers stored
patience = 100
# ROI area color transparency
alpha = 0.1

#@title 4: Read video
# Reading video with cv2
video = cv2.VideoCapture(path)

#@title 4.1: Set person class to detect in YOLO
# Objects to detect Yolo
class_IDS = [0]
# Auxiliary variables
centers_old = {}
obj_id = 0
end = []
frames_list = []
count_p = 0
lastKey = ''
print(f'[INFO] - Verbose during Prediction: {verbose}')

#@title 4.2: Settings for input of video
# Original informations of video
height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
fps = video.get(cv2.CAP_PROP_FPS)
print('[INFO] - Original Dim: ', (width, height))

# Scaling Video for better performance
if scale_percent != 100:
    print('[INFO] - Scaling change may cause errors in pixels lines ')
    width = int(width * scale_percent / 100)
    height = int(height * scale_percent / 100)
    print('[INFO] - Dim Scaled: ', (width, height))

#@title 4.3: Settings on video output
### Video output ####
video_name = 'result.mp4'
output_path = "rep_" + video_name
tmp_output_path = "tmp_" + output_path
VIDEO_CODEC = "MP4V" #set to MP4 codec

output_video = cv2.VideoWriter(tmp_output_path,
                               cv2.VideoWriter_fourcc(*VIDEO_CODEC),
                               fps, (width, height))

#@title 5: Executing Recognition in an ROI section
for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):

    # reading frame from video
    _, frame = video.read()

    # Applying resizing of read frame
    # Making a copy of the original color frame for later display
    frame  = risize_frame(frame, scale_percent)
    original_color_frame = frame.copy() # Make copy of color for later display
    frame  = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert to gray scale
    area_roi = [np.array([ (1800, 0),(1100,0),(0,0) ,(1800,0)], np.int32)]# ROI of entire Camera capture
    ROI = frame[100:1000, 100:1200] 

    if verbose:
        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))

    # Getting predictions
    y_hat = model.predict(ROI, conf = conf_level, classes = class_IDS, device = 0, verbose = False)

    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.
    boxes_data = y_hat[0].boxes.xyxy.cpu().numpy()
    confidences = y_hat[0].boxes.conf.cpu().numpy()
    classes = y_hat[0].boxes.cls.cpu().numpy()

    # Storing the above information in a dataframe, setting the min and max values
    positions_frame = pd.DataFrame(boxes_data, columns=['xmin', 'ymin', 'xmax', 'ymax'])
    positions_frame['conf'] = confidences
    positions_frame['class'] = classes

    #Translating the numeric class labels to text
    labels = [dict_classes[i] for i in classes]

    # For each people, draw the bounding-box and counting each one the pass thought the ROI area
    for ix, row in enumerate(positions_frame.iterrows()):
        # Getting the coordinates of each vehicle (row)
        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')

        # Calculating the center of the bounding-box
        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)

        #Updating the tracking for each object
        centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x, center_y), thr_centers, lastKey, i, frame_max)

        #Updating people in ROI
        count_p+=is_new

        # drawing center and bounding-box in the given frame
        cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0,0,255), 2)
        for center_x,center_y in centers_old[id_obj].values():
            cv2.circle(ROI, (center_x,center_y), 5,(0,0,255),-1)

        #Drawing above the bounding-box the name of class recognized.
        cv2.putText(img=ROI, text=id_obj+':'+str(np.round(confidences[ix],2)),
                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.9, color=(0, 0, 255),thickness=1)

    #drawing the number of people
    cv2.putText(img=frame, text=f'No. of People in ROI: {count_p}',
                org= (30,40), fontFace=cv2.FONT_HERSHEY_TRIPLEX,
                fontScale=1.5, color=(255, 0, 0), thickness=1)

    # Filtering tracks history
    centers_old = filter_tracks(centers_old, patience)
    if verbose:
        print(contador_in, contador_out)

    #Drawing the ROI area
    overlay = frame.copy()

    cv2.polylines(overlay, pts = area_roi, isClosed = True, color=(255, 0, 0),thickness=2)
    cv2.fillPoly(overlay, area_roi, (255,0,0))
    frame = cv2.addWeighted(overlay, alpha,frame , 1 - alpha, 0)

    #Saving frames in a list
    frames_list.append(frame)
    #saving transformed frames in a output video formaat
    output_video.write(frame)

#Releasing the video
output_video.release()

#@title 5.1: Process Video and Audio
# Check for Existing Output File and Remove if Present
if os.path.exists(output_path):
    os.remove(output_path)
# Process video and audio file
subprocess.run(
    ["ffmpeg",  "-i", tmp_output_path,"-crf","18","-preset","veryfast","-hide_banner","-loglevel","error","-vcodec","libx264",output_path])
os.remove(tmp_output_path)

#@title 5.2: Display of people and results
for i in [50, 51, 52, 53, 54, 55, 56, 70, 71, 72, 73]:
    plt.figure(figsize =( 7, 5))
    plt.imshow(frames_list[i])
    plt.show()

Questions:
1. I noticed it doesn't manage to capture and count many people in the image, especially on the right hand side where the scale is > 1250, why?
2. In the start of the image processing should I have added a Gaussian Blur?
3. At the end where I display the results can I display the frames in color?
4. Is there a report I can print out showing the number of people counted in intervals of 5 seconds, along with the confidence score, and maybe even direction of the movement?
5. Any suggestions how to make this better?
